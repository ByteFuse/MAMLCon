{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/training_results.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_df_1 = df[(df.n_way==50)&(df.n_class_additions==5)&(df.n_classes_start==5)&(df.algorithm=='FSCL')&(df.dataset=='flickr8k')].\\\n",
    "#     drop(columns=['Name']).groupby(['k_shot']).max().reset_index(drop=False)\n",
    "# plot_df_2 = df[(df.n_way==10)&(df.n_class_additions==1)&(df.n_classes_start==2)&(df.algorithm=='FSCL')&(df.dataset=='google_commands')].\\\n",
    "#     drop(columns=['Name']).groupby(['k_shot']).max().reset_index(drop=False)\n",
    "# plot_df = pd.concat([plot_df_1, plot_df_2])\n",
    "\n",
    "# def rename_(name):\n",
    "#     if name == 'flickr8k':\n",
    "#         return 'Flickr8k: N=50, CA=5, CS=5'\n",
    "#     else:\n",
    "#         return \"Google Commands: N=10, CA=2, CS=2\"\n",
    "\n",
    "# plot_df['dataset']=plot_df['dataset'].apply(rename_)\n",
    "# plot_df.rename(columns={'k_shot':'K-Shot', 'validation_query_accuracy_epoch':'Final Accuracy', 'dataset':'Dataset'}, inplace=True)\n",
    "\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# # set colours allowed to use to certain hex value\n",
    "# sns.set_palette(sns.color_palette([\"#1D2140\", \"#17C37B\"])) \n",
    "# sns.barplot(x='K-Shot', y='Final Accuracy', hue='Dataset', data=plot_df)\n",
    "# #add the value on top of the bar\n",
    "# for p in plt.gca().patches:\n",
    "#     plt.gca().text(p.get_x() + p.get_width()/2., p.get_height(), '{:1.2f}'.format(p.get_height()*100), fontsize=18, color='black', ha='center', va='bottom')\n",
    "\n",
    "# # make the legend text bigger\n",
    "# plt.legend(loc='lower right', prop={'size': 20})\n",
    "# # make plot font size bigger\n",
    "# plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "# # remove top grid line\n",
    "# plt.gca().spines['top'].set_visible(False)\n",
    "# plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# plt.savefig('k_shot_accuracy.png', transparent=True, pad_inches=.2, frameon=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from src.models import WordClassificationAudio2DCnn, WordClassificationAudioCnnPool as WordClassificationAudioCnn, WordClassificationRnn\n",
    "from src.losses import ClassificationLoss\n",
    "from src.algorithms import FSCL, OML\n",
    "from src.data.datasets import Flickr8kWordClassification, GoogleCommandsWordClassification\n",
    "from src.data.samplers import SpokenWordTaskBatchSampler\n",
    "from src.utils import flatten_dict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruanv\\AppData\\Local\\Temp\\ipykernel_12144\\1303192644.py:27: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(layer.weight, )\n"
     ]
    }
   ],
   "source": [
    "optim = {\n",
    "'name': 'adam',\n",
    "'inner_steps': 5,\n",
    "'val_inner_steps': 5,\n",
    "'gradient_clip_val': 0,\n",
    "'inner_learning_rate': 0.001,\n",
    "'outer_learning_rate': 0.0001,\n",
    "'scheduler': False,\n",
    "'scheduler_step': 30,\n",
    "'scheduler_decay': 0.1,\n",
    "}\n",
    "\n",
    "encoder = WordClassificationAudio2DCnn(\n",
    "    256, \n",
    "    64, \n",
    "    input_channels=39\n",
    ")\n",
    "\n",
    "class FSCLModel(nn.Module):\n",
    "    def __init__(self, encoder, embedding_dim, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        def return_classification_layer(embedding_dim):\n",
    "            layer = nn.Linear(embedding_dim, 1)\n",
    "            torch.nn.init.xavier_uniform(layer.weight, )\n",
    "            layer = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                layer\n",
    "            )\n",
    "            return layer\n",
    "\n",
    "        layers = [return_classification_layer(embedding_dim) for _ in range(n_classes)]\n",
    "        self.classifiers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, audio, total_classes_present):\n",
    "        features = self.encoder(audio)\n",
    "        layer_logits = []\n",
    "        for c_layer in range(total_classes_present):\n",
    "            layer_logits.append(self.classifiers[c_layer](features))\n",
    "        logits = torch.cat(layer_logits, dim=1)\n",
    "        return {'logits':logits}\n",
    "\n",
    "\n",
    "loss_fn = ClassificationLoss()\n",
    "\n",
    "model = FSCLModel(encoder, 256, 50)\n",
    "algorithm = FSCL.load_from_checkpoint(\n",
    "    './nway50_k5.ckpt', \n",
    "    model=model, \n",
    "    training_steps=5,\n",
    "    intial_training_steps=30,\n",
    "    n_classes_start=5,\n",
    "    n_class_additions=5,\n",
    "    loss_func=loss_fn,\n",
    "    optim_config=optim,\n",
    "    k_shot=5,\n",
    "    quick_adapt=True\n",
    ")\n",
    "model = algorithm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/ruanv/Desktop/speech-fewshot-cl/data/flickr/flickr8k_word_splits_validation.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audio: 100%|██████████| 32974/32974 [00:00<00:00, 363321.31it/s]\n",
      "Loading audio: 100%|██████████| 2500/2500 [00:00<00:00, 279188.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open('../config/conversion_method/mfcc.yaml') as f:\n",
    "    conv_config = yaml.safe_load(f) \n",
    "\n",
    "valiadation_dataset = Flickr8kWordClassification(\n",
    "    meta_path='C:/Users/ruanv/Desktop/speech-fewshot-cl/data/flickr/flickr8k_word_splits_validation.csv',\n",
    "    audio_root='C:/Users/ruanv/Desktop/speech-fewshot-cl/data/flickr/wavs/', \n",
    "    conversion_config=conv_config,\n",
    "    stemming=True, \n",
    "    lemmetise=False          \n",
    ")\n",
    "\n",
    "\n",
    "def pad_audio(x, max_audio_len=101, pad_both_sides=False):\n",
    "    import torch.nn.functional as F\n",
    "    if x.size(-1) > max_audio_len:\n",
    "        x = x[:,:max_audio_len]\n",
    "    else:\n",
    "        if pad_both_sides:\n",
    "            pad_lenght = int(max_audio_len-x.size(-1))//2\n",
    "            x = F.pad(x, (pad_lenght, pad_lenght+1 if int(max_audio_len-x.size(-1))%2!=0 else pad_lenght), 'constant', 0)\n",
    "        else:\n",
    "            x = F.pad(x, (0, int(max_audio_len-x.size(-1))), 'constant', 0)\n",
    "    return x\n",
    "\n",
    "df = pd.read_csv('C:/Users/ruanv/Desktop/speech-fewshot-cl/data/flickr/flickr8k_word_splits_validation.csv')\n",
    "words_keep = [\n",
    "'surf', 'wave', 'crowd', 'fight', 'togeth', 'ski',\n",
    "'bridg', 'wood', 'teenag', 'lot', 'mud', 'pool', 'tabl', 'out',\n",
    "'wet', 'footbal', 'make', 'team', 'shop', 'their', 'edg', 'guitar',\n",
    "'across', 'area', 'do', 'trick', 'bike', 'obstacl', 'tri', 'rider',\n",
    "'track', 'room', 'him', 'jacket', 'glass', 'open', 'them', 'cap',\n",
    "'color', 'set', 'pant', 'wrestl', 'basketbal', 'climber', 'face',\n",
    "'mountain', 'tent', 'shore', 'ground', 'bar'\n",
    "]\n",
    "df = df[df.stem.isin(words_keep)]\n",
    "\n",
    "K = 20\n",
    "\n",
    "# sample 5 instances of each stem\n",
    "df = df.groupby('stem').apply(lambda x: x.sample(K)).reset_index(drop=True)\n",
    "df.to_csv('./plot_data_words.csv', index=False)\n",
    "\n",
    "\n",
    "valiadation_dataset = Flickr8kWordClassification(\n",
    "    meta_path='plot_data_words.csv',\n",
    "    audio_root='C:/Users/ruanv/Desktop/speech-fewshot-cl/data/flickr/wavs/', \n",
    "    conversion_config=conv_config,\n",
    "    stemming=True, \n",
    "    lemmetise=False          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valiadation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "STEPS = K*N\n",
    "\n",
    "\n",
    "groups = []\n",
    "test_groups = []\n",
    "\n",
    "for i in range(0, len(valiadation_dataset), STEPS):\n",
    "    arrays = []\n",
    "    labels = []\n",
    "\n",
    "    for j in range(0+i, STEPS+i):\n",
    "        arrays.append(pad_audio(torch.tensor(valiadation_dataset[j][0])).unsqueeze(0))\n",
    "        labels.append(torch.tensor(valiadation_dataset[j][1]))\n",
    "    \n",
    "    arrays = torch.concat(arrays,dim=0)\n",
    "\n",
    "    # take every 5th instance as test\n",
    "    test_arrays = arrays[::5]\n",
    "    test_labels = labels[::5]\n",
    "\n",
    "    # skip every 5th instance\n",
    "    arrays = torch.concat([array.unsqueeze(0) for i, array in enumerate(arrays) if i%5!=0 or i==0],dim=0)\n",
    "    labels = [label for i, label in enumerate(labels) if i%5!=0 or i==0]\n",
    "\n",
    "    groups.append({'data':arrays, 'labels':labels})\n",
    "    test_groups.append({'data':test_arrays, 'labels':test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruanv\\AppData\\Local\\Temp\\ipykernel_12144\\1303192644.py:27: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(layer.weight, )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     29\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m outputs \u001b[39m=\u001b[39m model(data, (n\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m*\u001b[39;49mN)\n\u001b[0;32m     31\u001b[0m outputs[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(labels)\n\u001b[0;32m     32\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\learn2learn\\algorithms\\maml.py:107\u001b[0m, in \u001b[0;36mMAML.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [3], line 38\u001b[0m, in \u001b[0;36mFSCLModel.forward\u001b[1;34m(self, audio, total_classes_present)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, audio, total_classes_present):\n\u001b[1;32m---> 38\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(audio)\n\u001b[0;32m     39\u001b[0m     layer_logits \u001b[39m=\u001b[39m []\n\u001b[0;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m c_layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_classes_present):\n",
      "File \u001b[1;32mc:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\ruanv\\desktop\\speech-fewshot-cl\\src\\models.py:102\u001b[0m, in \u001b[0;36mWordClassificationAudio2DCnn.forward\u001b[1;34m(self, audio)\u001b[0m\n\u001b[0;32m    100\u001b[0m     audio \u001b[39m=\u001b[39m audio\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder:\n\u001b[1;32m--> 102\u001b[0m     audio \u001b[39m=\u001b[39m conv(audio)\n\u001b[0;32m    104\u001b[0m audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(audio)\n\u001b[0;32m    105\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection(audio)\n",
      "File \u001b[1;32mc:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    177\u001b[0m     bn_training,\n\u001b[0;32m    178\u001b[0m     exponential_average_factor,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    180\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ruanv\\miniconda3\\envs\\flickr2\\lib\\site-packages\\torch\\nn\\functional.py:2282\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2279\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2280\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2282\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2283\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2284\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "model = FSCLModel(encoder, 256, 50)\n",
    "algorithm = FSCL.load_from_checkpoint(\n",
    "    './nway50_k5.ckpt', \n",
    "    model=model, \n",
    "    training_steps=5,\n",
    "    intial_training_steps=30,\n",
    "    n_classes_start=5,\n",
    "    n_class_additions=5,\n",
    "    loss_func=loss_fn,\n",
    "    optim_config=optim,\n",
    "    k_shot=5,\n",
    "    quick_adapt=True\n",
    ")\n",
    "model = algorithm.model\n",
    "\n",
    "criterion = ClassificationLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for n, group in enumerate(groups):\n",
    "    model.train()\n",
    "    data, labels = group['data'], group['labels']\n",
    "    for i in range(50):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data, (n+1)*N)\n",
    "        outputs['labels'] = torch.tensor(labels)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    # Print loss for each epoch\n",
    "    print(f'Training loss: {loss.item():.4f}')\n",
    "\n",
    "    for group in groups[:n+1]:\n",
    "        data, labels = group['data'], group['labels']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # take every 5th instance\n",
    "        data = data[::5]\n",
    "        labels = labels[::5]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data, (n+1)*N)\n",
    "        outputs['labels'] = torch.tensor(labels)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for i, group in enumerate(test_groups[:n+1]):\n",
    "            data, labels = group['data'], group['labels']\n",
    "            outputs = model(data, (i+1)*N)\n",
    "            outputs['labels'] = torch.tensor(labels)\n",
    "            loss = criterion(outputs)\n",
    "\n",
    "            predicted_labels = torch.argmax(outputs['logits'], dim=1)\n",
    "            accuracy = torch.sum(predicted_labels == torch.tensor(labels)).item() / len(labels)\n",
    "\n",
    "            # print(f'Test loss for group {i}, after training group {n}: {loss.item():.4f}')\n",
    "            # print(f'Test accuracy for group {i}, after training group {n}: {accuracy:.4f}') \n",
    "            # print('---')\n",
    "\n",
    "            results = pd.concat([results,pd.DataFrame({'loss':[loss.item()], 'accuracy':[accuracy], 'group':[i], 'training_group':[n]})])\n",
    "            results.to_csv('group_averages_MAMLCon_N10.csv', index=False)\n",
    "\n",
    "    # print('*'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='group', ylabel='accuracy'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(x='group', y='accuracy', hue='training_group', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruanv\\AppData\\Local\\Temp\\ipykernel_17780\\1303192644.py:27: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(layer.weight, )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0008\n",
      "Training loss: 0.0014\n",
      "Training loss: 0.0016\n",
      "Training loss: 0.0023\n",
      "Training loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "t_model = FSCLModel(encoder, 256, 50)\n",
    "results = pd.DataFrame()\n",
    "\n",
    "criterion = ClassificationLoss()\n",
    "optimizer = torch.optim.Adam(t_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for n, group in enumerate(groups):\n",
    "    t_model.train()\n",
    "    data, labels = group['data'], group['labels']\n",
    "    for i in range(50):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = t_model(data, (n+1)*N)\n",
    "        outputs['labels'] = torch.tensor(labels)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    # Print loss for each epoch\n",
    "    print(f'Training loss: {loss.item():.4f}')\n",
    "\n",
    "    # for group in groups[:n+1]:\n",
    "    #     data, labels = group['data'], group['labels']\n",
    "    #     optimizer.zero_grad()\n",
    "\n",
    "    #     # take every 5th instance\n",
    "    #     data = data[::5]\n",
    "    #     labels = labels[::5]\n",
    "\n",
    "    #     # Forward pass\n",
    "    #     outputs = t_model(data, (n+1)*N)\n",
    "    #     outputs['labels'] = torch.tensor(labels)\n",
    "    #     # Compute loss\n",
    "    #     loss = criterion(outputs)\n",
    "        \n",
    "    #     # Backward pass and optimization\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t_model.eval()\n",
    "\n",
    "        for i, group in enumerate(test_groups[:n+1]):\n",
    "            data, labels = group['data'], group['labels']\n",
    "            outputs = t_model(data, (i+1)*N)\n",
    "            outputs['labels'] = torch.tensor(labels)\n",
    "            loss = criterion(outputs)\n",
    "\n",
    "            predicted_labels = torch.argmax(outputs['logits'], dim=1)\n",
    "            accuracy = torch.sum(predicted_labels == torch.tensor(labels)).item() / len(labels)\n",
    "\n",
    "            # print(f'Test loss for group {i}, after training group {n}: {loss.item():.4f}')\n",
    "            # print(f'Test accuracy for group {i}, after training group {n}: {accuracy:.4f}') \n",
    "            # print('---')\n",
    "\n",
    "            results = pd.concat([results,pd.DataFrame({'loss':[loss.item()], 'accuracy':[accuracy], 'group':[i], 'training_group':[n]})])\n",
    "            results.to_csv('group_averages_no_update_N10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flickr2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a43d7b48dfb8db078ab7642bd2e0d3611ffe6518b405d3c07c6468471a4b930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
